{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab441572",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "36f05abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import os\n",
    "\n",
    "import arff\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from typing import Optional, List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ba618e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add current folder to sys.path so Python sees \"tabicl\"\n",
    "sys.path.insert(0, os.path.abspath(\".\"))\n",
    "\n",
    "from tabicl.src.tabicl.model.tabicl import TabICL\n",
    "from tabicl.src.tabicl.sklearn.classifier import TabICLClassifier\n",
    "from tabicl.src.tabicl.sklearn.preprocessing import (\n",
    "    TransformToNumerical,\n",
    "    EnsembleGenerator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e28856b",
   "metadata": {},
   "source": [
    "### Load and process dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e0fadd36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        V2       V3   V4 V5      V6      V7         V8       V9  \\\n",
      "0     Sexy      Low  4.6  M  Summer  o-neck  sleevless   empire   \n",
      "1   Casual      Low  0.0  L  Summer  o-neck      Petal  natural   \n",
      "2  vintage     High  0.0  L  Automn  o-neck       full  natural   \n",
      "3    Brief  Average  4.6  L  Spring  o-neck       full  natural   \n",
      "4     cute      Low  4.5  M  Summer  o-neck  butterfly  natural   \n",
      "\n",
      "             V10      V11         V12     V13 Class  \n",
      "0           None  chiffon     ruffles  animal     2  \n",
      "1     microfiber     None     ruffles  animal     1  \n",
      "2       polyster     None        None   print     1  \n",
      "3           silk  chiffon  embroidary   print     2  \n",
      "4  chiffonfabric  chiffon         bow     dot     1  \n"
     ]
    }
   ],
   "source": [
    "with open(\"data.arff\") as f:\n",
    "    dataset = arff.load(f)\n",
    "\n",
    "# dataset['data'] is a generator, convert it to a list\n",
    "data_list = list(dataset[\"data\"])\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(dataset[\"data\"], columns=[attr[0] for attr in dataset[\"attributes\"]])\n",
    "# df = pd.DataFrame(dataset[\"data\"], columns=[attr[0] for attr in dataset[\"attributes\"]])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ce3599da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 500\n",
      "Number of columns: 13\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of rows: {df.shape[0]}\")\n",
    "print(f\"Number of columns: {df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a50483de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (400, 12) (400,)\n",
      "Val:   (50, 12) (50,)\n",
      "Test:  (50, 12) (50,)\n",
      "Classes:  ['2' '1']\n"
     ]
    }
   ],
   "source": [
    "# Separate features and target\n",
    "X = df.drop(columns=[\"Class\"])\n",
    "y = df[\"Class\"]\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(\"Train:\", X_train.shape, y_train.shape)\n",
    "print(\"Val:  \", X_val.shape, y_val.shape)\n",
    "print(\"Test: \", X_test.shape, y_test.shape)\n",
    "\n",
    "print(\"Classes: \", y.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0a8e5f",
   "metadata": {},
   "source": [
    "### Data processing for finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "aca1bc79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>EnsembleGenerator(class_shift=(True,), n_estimators=32,\n",
       "                  norm_methods=[&#x27;none&#x27;, &#x27;power&#x27;], outlier_threshold=(4.0,))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;EnsembleGenerator<span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>EnsembleGenerator(class_shift=(True,), n_estimators=32,\n",
       "                  norm_methods=[&#x27;none&#x27;, &#x27;power&#x27;], outlier_threshold=(4.0,))</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "EnsembleGenerator(class_shift=(True,), n_estimators=32,\n",
       "                  norm_methods=['none', 'power'], outlier_threshold=(4.0,))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_encoder_ = LabelEncoder()\n",
    "y_train_pr = y_encoder_.fit_transform(y_train)\n",
    "y_val_pr = y_encoder_.fit_transform(y_val)\n",
    "classes_ = y_encoder_.classes_\n",
    "n_classes_ = len(y_encoder_.classes_)\n",
    "\n",
    "#  Transform input features\n",
    "X_encoder_ = TransformToNumerical(verbose=False)\n",
    "X_train_pr = X_encoder_.fit_transform(X_train)\n",
    "X_val_pr = X_encoder_.fit_transform(X_val)\n",
    "\n",
    "n_estimators: int = (32,)\n",
    "norm_methods: Optional[str | List[str]] = (None,)\n",
    "feat_shuffle_method: str = (\"latin\",)\n",
    "class_shift: bool = (True,)\n",
    "outlier_threshold: float = (4.0,)\n",
    "softmax_temperature: float = (0.9,)\n",
    "average_logits: bool = (True,)\n",
    "use_hierarchical: bool = True\n",
    "random_state: int | None = (42,)\n",
    "\n",
    "seed = random_state if isinstance(random_state, (int, type(None))) else None\n",
    "\n",
    "# Fit ensemble generator to create multiple dataset views\n",
    "ensemble_generator_ = EnsembleGenerator(\n",
    "    n_estimators=32,\n",
    "    norm_methods=[\"none\", \"power\"],\n",
    "    feat_shuffle_method=\"latin\",\n",
    "    class_shift=class_shift,\n",
    "    outlier_threshold=outlier_threshold,\n",
    "    random_state=seed,\n",
    ")\n",
    "ensemble_generator_.fit(X_train_pr, y_train_pr)\n",
    "ensemble_generator_.fit(X_val_pr, y_val_pr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7be91ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train tensor shape:  torch.Size([400, 12])\n",
      "y train tensor shape:  torch.Size([400])\n",
      "X train tensor shape:  torch.Size([50, 12])\n",
      "y train tensor shape:  torch.Size([50])\n"
     ]
    }
   ],
   "source": [
    "X_train_tensor = torch.tensor(X_train_pr, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_pr, dtype=torch.long)\n",
    "\n",
    "# X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "# y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# X_train_tensor = X_train_tensor.unsqueeze(0)\n",
    "# y_train_tensor = y_train_tensor.unsqueeze(0)\n",
    "\n",
    "print(\"X train tensor shape: \", X_train_tensor.shape)\n",
    "print(\"y train tensor shape: \", y_train_tensor.shape)\n",
    "\n",
    "X_val_tensor = torch.tensor(X_val_pr, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val_pr, dtype=torch.long)\n",
    "\n",
    "# X_val_tensor = X_val_tensor.unsqueeze(0)\n",
    "# y_val_tensor = y_val_tensor.unsqueeze(0)\n",
    "\n",
    "print(\"X train tensor shape: \", X_val_tensor.shape)\n",
    "print(\"y train tensor shape: \", y_val_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95d0be4",
   "metadata": {},
   "source": [
    "### Load model and weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f492c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters:  27051666\n"
     ]
    }
   ],
   "source": [
    "model = TabICL()\n",
    "\n",
    "\n",
    "def cnt_params(model):\n",
    "    return sum(param.numel() for param in model.parameters())\n",
    "\n",
    "\n",
    "print(\"Number of parameters: \", cnt_params(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdef7b57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ColEmbedding(\n",
       "  (in_linear): SkippableLinear(in_features=1, out_features=128, bias=True)\n",
       "  (tf_col): SetTransformer(\n",
       "    (blocks): ModuleList(\n",
       "      (0-2): 3 x InducedSelfAttentionBlock(\n",
       "        (multihead_attn1): MultiheadAttentionBlock(\n",
       "          (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (linear2): Linear(in_features=256, out_features=128, bias=True)\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.0, inplace=False)\n",
       "          (dropout2): Dropout(p=0.0, inplace=False)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (multihead_attn2): MultiheadAttentionBlock(\n",
       "          (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (linear2): Linear(in_features=256, out_features=128, bias=True)\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.0, inplace=False)\n",
       "          (dropout2): Dropout(p=0.0, inplace=False)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (out_w): SkippableLinear(in_features=128, out_features=128, bias=True)\n",
       "  (ln_w): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (out_b): SkippableLinear(in_features=128, out_features=128, bias=True)\n",
       "  (ln_b): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.col_embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94bcb358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module:  | Params: 27051658\n",
      "Module: col_embedder | Params: 877824\n",
      "Module: col_embedder.in_linear | Params: 256\n",
      "Module: col_embedder.tf_col | Params: 844032\n",
      "Module: col_embedder.tf_col.blocks | Params: 844032\n",
      "Module: col_embedder.tf_col.blocks.0 | Params: 281344\n",
      "Module: col_embedder.tf_col.blocks.0.multihead_attn1 | Params: 132480\n",
      "Module: col_embedder.tf_col.blocks.0.multihead_attn1.linear1 | Params: 33024\n",
      "Module: col_embedder.tf_col.blocks.0.multihead_attn1.linear2 | Params: 32896\n",
      "Module: col_embedder.tf_col.blocks.0.multihead_attn1.norm1 | Params: 256\n",
      "Module: col_embedder.tf_col.blocks.0.multihead_attn1.norm2 | Params: 256\n",
      "Module: col_embedder.tf_col.blocks.0.multihead_attn1.attn | Params: 66048\n",
      "Module: col_embedder.tf_col.blocks.0.multihead_attn1.attn.out_proj | Params: 16512\n",
      "Module: col_embedder.tf_col.blocks.0.multihead_attn2 | Params: 132480\n",
      "Module: col_embedder.tf_col.blocks.0.multihead_attn2.linear1 | Params: 33024\n",
      "Module: col_embedder.tf_col.blocks.0.multihead_attn2.linear2 | Params: 32896\n",
      "Module: col_embedder.tf_col.blocks.0.multihead_attn2.norm1 | Params: 256\n",
      "Module: col_embedder.tf_col.blocks.0.multihead_attn2.norm2 | Params: 256\n",
      "Module: col_embedder.tf_col.blocks.0.multihead_attn2.attn | Params: 66048\n",
      "Module: col_embedder.tf_col.blocks.0.multihead_attn2.attn.out_proj | Params: 16512\n",
      "Module: col_embedder.tf_col.blocks.1 | Params: 281344\n",
      "Module: col_embedder.tf_col.blocks.1.multihead_attn1 | Params: 132480\n",
      "Module: col_embedder.tf_col.blocks.1.multihead_attn1.linear1 | Params: 33024\n",
      "Module: col_embedder.tf_col.blocks.1.multihead_attn1.linear2 | Params: 32896\n",
      "Module: col_embedder.tf_col.blocks.1.multihead_attn1.norm1 | Params: 256\n",
      "Module: col_embedder.tf_col.blocks.1.multihead_attn1.norm2 | Params: 256\n",
      "Module: col_embedder.tf_col.blocks.1.multihead_attn1.attn | Params: 66048\n",
      "Module: col_embedder.tf_col.blocks.1.multihead_attn1.attn.out_proj | Params: 16512\n",
      "Module: col_embedder.tf_col.blocks.1.multihead_attn2 | Params: 132480\n",
      "Module: col_embedder.tf_col.blocks.1.multihead_attn2.linear1 | Params: 33024\n",
      "Module: col_embedder.tf_col.blocks.1.multihead_attn2.linear2 | Params: 32896\n",
      "Module: col_embedder.tf_col.blocks.1.multihead_attn2.norm1 | Params: 256\n",
      "Module: col_embedder.tf_col.blocks.1.multihead_attn2.norm2 | Params: 256\n",
      "Module: col_embedder.tf_col.blocks.1.multihead_attn2.attn | Params: 66048\n",
      "Module: col_embedder.tf_col.blocks.1.multihead_attn2.attn.out_proj | Params: 16512\n",
      "Module: col_embedder.tf_col.blocks.2 | Params: 281344\n",
      "Module: col_embedder.tf_col.blocks.2.multihead_attn1 | Params: 132480\n",
      "Module: col_embedder.tf_col.blocks.2.multihead_attn1.linear1 | Params: 33024\n",
      "Module: col_embedder.tf_col.blocks.2.multihead_attn1.linear2 | Params: 32896\n",
      "Module: col_embedder.tf_col.blocks.2.multihead_attn1.norm1 | Params: 256\n",
      "Module: col_embedder.tf_col.blocks.2.multihead_attn1.norm2 | Params: 256\n",
      "Module: col_embedder.tf_col.blocks.2.multihead_attn1.attn | Params: 66048\n",
      "Module: col_embedder.tf_col.blocks.2.multihead_attn1.attn.out_proj | Params: 16512\n",
      "Module: col_embedder.tf_col.blocks.2.multihead_attn2 | Params: 132480\n",
      "Module: col_embedder.tf_col.blocks.2.multihead_attn2.linear1 | Params: 33024\n",
      "Module: col_embedder.tf_col.blocks.2.multihead_attn2.linear2 | Params: 32896\n",
      "Module: col_embedder.tf_col.blocks.2.multihead_attn2.norm1 | Params: 256\n",
      "Module: col_embedder.tf_col.blocks.2.multihead_attn2.norm2 | Params: 256\n",
      "Module: col_embedder.tf_col.blocks.2.multihead_attn2.attn | Params: 66048\n",
      "Module: col_embedder.tf_col.blocks.2.multihead_attn2.attn.out_proj | Params: 16512\n",
      "Module: col_embedder.out_w | Params: 16512\n",
      "Module: col_embedder.ln_w | Params: 256\n",
      "Module: col_embedder.out_b | Params: 16512\n",
      "Module: col_embedder.ln_b | Params: 256\n",
      "Module: row_interactor | Params: 398208\n",
      "Module: row_interactor.tf_row | Params: 397440\n",
      "Module: row_interactor.tf_row.blocks | Params: 397440\n",
      "Module: row_interactor.tf_row.blocks.0 | Params: 132480\n",
      "Module: row_interactor.tf_row.blocks.0.linear1 | Params: 33024\n",
      "Module: row_interactor.tf_row.blocks.0.linear2 | Params: 32896\n",
      "Module: row_interactor.tf_row.blocks.0.norm1 | Params: 256\n",
      "Module: row_interactor.tf_row.blocks.0.norm2 | Params: 256\n",
      "Module: row_interactor.tf_row.blocks.0.attn | Params: 66048\n",
      "Module: row_interactor.tf_row.blocks.0.attn.out_proj | Params: 16512\n",
      "Module: row_interactor.tf_row.blocks.1 | Params: 132480\n",
      "Module: row_interactor.tf_row.blocks.1.linear1 | Params: 33024\n",
      "Module: row_interactor.tf_row.blocks.1.linear2 | Params: 32896\n",
      "Module: row_interactor.tf_row.blocks.1.norm1 | Params: 256\n",
      "Module: row_interactor.tf_row.blocks.1.norm2 | Params: 256\n",
      "Module: row_interactor.tf_row.blocks.1.attn | Params: 66048\n",
      "Module: row_interactor.tf_row.blocks.1.attn.out_proj | Params: 16512\n",
      "Module: row_interactor.tf_row.blocks.2 | Params: 132480\n",
      "Module: row_interactor.tf_row.blocks.2.linear1 | Params: 33024\n",
      "Module: row_interactor.tf_row.blocks.2.linear2 | Params: 32896\n",
      "Module: row_interactor.tf_row.blocks.2.norm1 | Params: 256\n",
      "Module: row_interactor.tf_row.blocks.2.norm2 | Params: 256\n",
      "Module: row_interactor.tf_row.blocks.2.attn | Params: 66048\n",
      "Module: row_interactor.tf_row.blocks.2.attn.out_proj | Params: 16512\n",
      "Module: row_interactor.out_ln | Params: 256\n",
      "Module: icl_predictor | Params: 25775626\n",
      "Module: icl_predictor.tf_icl | Params: 25233408\n",
      "Module: icl_predictor.tf_icl.blocks | Params: 25233408\n",
      "Module: icl_predictor.tf_icl.blocks.0 | Params: 2102784\n",
      "Module: icl_predictor.tf_icl.blocks.0.linear1 | Params: 525312\n",
      "Module: icl_predictor.tf_icl.blocks.0.linear2 | Params: 524800\n",
      "Module: icl_predictor.tf_icl.blocks.0.norm1 | Params: 1024\n",
      "Module: icl_predictor.tf_icl.blocks.0.norm2 | Params: 1024\n",
      "Module: icl_predictor.tf_icl.blocks.0.attn | Params: 1050624\n",
      "Module: icl_predictor.tf_icl.blocks.0.attn.out_proj | Params: 262656\n",
      "Module: icl_predictor.tf_icl.blocks.1 | Params: 2102784\n",
      "Module: icl_predictor.tf_icl.blocks.1.linear1 | Params: 525312\n",
      "Module: icl_predictor.tf_icl.blocks.1.linear2 | Params: 524800\n",
      "Module: icl_predictor.tf_icl.blocks.1.norm1 | Params: 1024\n",
      "Module: icl_predictor.tf_icl.blocks.1.norm2 | Params: 1024\n",
      "Module: icl_predictor.tf_icl.blocks.1.attn | Params: 1050624\n",
      "Module: icl_predictor.tf_icl.blocks.1.attn.out_proj | Params: 262656\n",
      "Module: icl_predictor.tf_icl.blocks.2 | Params: 2102784\n",
      "Module: icl_predictor.tf_icl.blocks.2.linear1 | Params: 525312\n",
      "Module: icl_predictor.tf_icl.blocks.2.linear2 | Params: 524800\n",
      "Module: icl_predictor.tf_icl.blocks.2.norm1 | Params: 1024\n",
      "Module: icl_predictor.tf_icl.blocks.2.norm2 | Params: 1024\n",
      "Module: icl_predictor.tf_icl.blocks.2.attn | Params: 1050624\n",
      "Module: icl_predictor.tf_icl.blocks.2.attn.out_proj | Params: 262656\n",
      "Module: icl_predictor.tf_icl.blocks.3 | Params: 2102784\n",
      "Module: icl_predictor.tf_icl.blocks.3.linear1 | Params: 525312\n",
      "Module: icl_predictor.tf_icl.blocks.3.linear2 | Params: 524800\n",
      "Module: icl_predictor.tf_icl.blocks.3.norm1 | Params: 1024\n",
      "Module: icl_predictor.tf_icl.blocks.3.norm2 | Params: 1024\n",
      "Module: icl_predictor.tf_icl.blocks.3.attn | Params: 1050624\n",
      "Module: icl_predictor.tf_icl.blocks.3.attn.out_proj | Params: 262656\n",
      "Module: icl_predictor.tf_icl.blocks.4 | Params: 2102784\n",
      "Module: icl_predictor.tf_icl.blocks.4.linear1 | Params: 525312\n",
      "Module: icl_predictor.tf_icl.blocks.4.linear2 | Params: 524800\n",
      "Module: icl_predictor.tf_icl.blocks.4.norm1 | Params: 1024\n",
      "Module: icl_predictor.tf_icl.blocks.4.norm2 | Params: 1024\n",
      "Module: icl_predictor.tf_icl.blocks.4.attn | Params: 1050624\n",
      "Module: icl_predictor.tf_icl.blocks.4.attn.out_proj | Params: 262656\n",
      "Module: icl_predictor.tf_icl.blocks.5 | Params: 2102784\n",
      "Module: icl_predictor.tf_icl.blocks.5.linear1 | Params: 525312\n",
      "Module: icl_predictor.tf_icl.blocks.5.linear2 | Params: 524800\n",
      "Module: icl_predictor.tf_icl.blocks.5.norm1 | Params: 1024\n",
      "Module: icl_predictor.tf_icl.blocks.5.norm2 | Params: 1024\n",
      "Module: icl_predictor.tf_icl.blocks.5.attn | Params: 1050624\n",
      "Module: icl_predictor.tf_icl.blocks.5.attn.out_proj | Params: 262656\n",
      "Module: icl_predictor.tf_icl.blocks.6 | Params: 2102784\n",
      "Module: icl_predictor.tf_icl.blocks.6.linear1 | Params: 525312\n",
      "Module: icl_predictor.tf_icl.blocks.6.linear2 | Params: 524800\n",
      "Module: icl_predictor.tf_icl.blocks.6.norm1 | Params: 1024\n",
      "Module: icl_predictor.tf_icl.blocks.6.norm2 | Params: 1024\n",
      "Module: icl_predictor.tf_icl.blocks.6.attn | Params: 1050624\n",
      "Module: icl_predictor.tf_icl.blocks.6.attn.out_proj | Params: 262656\n",
      "Module: icl_predictor.tf_icl.blocks.7 | Params: 2102784\n",
      "Module: icl_predictor.tf_icl.blocks.7.linear1 | Params: 525312\n",
      "Module: icl_predictor.tf_icl.blocks.7.linear2 | Params: 524800\n",
      "Module: icl_predictor.tf_icl.blocks.7.norm1 | Params: 1024\n",
      "Module: icl_predictor.tf_icl.blocks.7.norm2 | Params: 1024\n",
      "Module: icl_predictor.tf_icl.blocks.7.attn | Params: 1050624\n",
      "Module: icl_predictor.tf_icl.blocks.7.attn.out_proj | Params: 262656\n",
      "Module: icl_predictor.tf_icl.blocks.8 | Params: 2102784\n",
      "Module: icl_predictor.tf_icl.blocks.8.linear1 | Params: 525312\n",
      "Module: icl_predictor.tf_icl.blocks.8.linear2 | Params: 524800\n",
      "Module: icl_predictor.tf_icl.blocks.8.norm1 | Params: 1024\n",
      "Module: icl_predictor.tf_icl.blocks.8.norm2 | Params: 1024\n",
      "Module: icl_predictor.tf_icl.blocks.8.attn | Params: 1050624\n",
      "Module: icl_predictor.tf_icl.blocks.8.attn.out_proj | Params: 262656\n",
      "Module: icl_predictor.tf_icl.blocks.9 | Params: 2102784\n",
      "Module: icl_predictor.tf_icl.blocks.9.linear1 | Params: 525312\n",
      "Module: icl_predictor.tf_icl.blocks.9.linear2 | Params: 524800\n",
      "Module: icl_predictor.tf_icl.blocks.9.norm1 | Params: 1024\n",
      "Module: icl_predictor.tf_icl.blocks.9.norm2 | Params: 1024\n",
      "Module: icl_predictor.tf_icl.blocks.9.attn | Params: 1050624\n",
      "Module: icl_predictor.tf_icl.blocks.9.attn.out_proj | Params: 262656\n",
      "Module: icl_predictor.tf_icl.blocks.10 | Params: 2102784\n",
      "Module: icl_predictor.tf_icl.blocks.10.linear1 | Params: 525312\n",
      "Module: icl_predictor.tf_icl.blocks.10.linear2 | Params: 524800\n",
      "Module: icl_predictor.tf_icl.blocks.10.norm1 | Params: 1024\n",
      "Module: icl_predictor.tf_icl.blocks.10.norm2 | Params: 1024\n",
      "Module: icl_predictor.tf_icl.blocks.10.attn | Params: 1050624\n",
      "Module: icl_predictor.tf_icl.blocks.10.attn.out_proj | Params: 262656\n",
      "Module: icl_predictor.tf_icl.blocks.11 | Params: 2102784\n",
      "Module: icl_predictor.tf_icl.blocks.11.linear1 | Params: 525312\n",
      "Module: icl_predictor.tf_icl.blocks.11.linear2 | Params: 524800\n",
      "Module: icl_predictor.tf_icl.blocks.11.norm1 | Params: 1024\n",
      "Module: icl_predictor.tf_icl.blocks.11.norm2 | Params: 1024\n",
      "Module: icl_predictor.tf_icl.blocks.11.attn | Params: 1050624\n",
      "Module: icl_predictor.tf_icl.blocks.11.attn.out_proj | Params: 262656\n",
      "Module: icl_predictor.ln | Params: 1024\n",
      "Module: icl_predictor.y_encoder | Params: 5632\n",
      "Module: icl_predictor.decoder | Params: 535562\n",
      "Module: icl_predictor.decoder.0 | Params: 525312\n",
      "Module: icl_predictor.decoder.2 | Params: 10250\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    params = sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "    if params > 0:\n",
    "        print(f\"Module: {name} | Params: {params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c3564fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_71674/801966743.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TabICL(\n",
       "  (col_embedder): ColEmbedding(\n",
       "    (in_linear): SkippableLinear(in_features=1, out_features=128, bias=True)\n",
       "    (tf_col): SetTransformer(\n",
       "      (blocks): ModuleList(\n",
       "        (0-2): 3 x InducedSelfAttentionBlock(\n",
       "          (multihead_attn1): MultiheadAttentionBlock(\n",
       "            (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (linear2): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.0, inplace=False)\n",
       "            (dropout2): Dropout(p=0.0, inplace=False)\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (multihead_attn2): MultiheadAttentionBlock(\n",
       "            (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (linear2): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.0, inplace=False)\n",
       "            (dropout2): Dropout(p=0.0, inplace=False)\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (out_w): SkippableLinear(in_features=128, out_features=128, bias=True)\n",
       "    (ln_w): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (out_b): SkippableLinear(in_features=128, out_features=128, bias=True)\n",
       "    (ln_b): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (row_interactor): RowInteraction(\n",
       "    (tf_row): Encoder(\n",
       "      (blocks): ModuleList(\n",
       "        (0-2): 3 x MultiheadAttentionBlock(\n",
       "          (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (linear2): Linear(in_features=256, out_features=128, bias=True)\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.0, inplace=False)\n",
       "          (dropout2): Dropout(p=0.0, inplace=False)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (rope): RotaryEmbedding()\n",
       "    )\n",
       "    (out_ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (icl_predictor): ICLearning(\n",
       "    (tf_icl): Encoder(\n",
       "      (blocks): ModuleList(\n",
       "        (0-11): 12 x MultiheadAttentionBlock(\n",
       "          (linear1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (linear2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.0, inplace=False)\n",
       "          (dropout2): Dropout(p=0.0, inplace=False)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (y_encoder): OneHotAndLinear(in_features=10, out_features=512, bias=True)\n",
       "    (decoder): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Linear(in_features=1024, out_features=10, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BASE_DIR = Path().resolve()\n",
    "checkpoint_path = BASE_DIR / \"tabicl-classifier-v1.1-0506.ckpt\"\n",
    "checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f46234",
   "metadata": {},
   "source": [
    "### Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "676ee56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, X_val, y_val, train_len, micro_d_val, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_train = y_val[:, :16].to(device)\n",
    "        y_test = y_val[:, 16:].to(device)\n",
    "        pred = model(X_val, y_train, micro_d_val)\n",
    "        pred = pred.flatten(end_dim=-2)\n",
    "        true = y_test.long().flatten()\n",
    "        _, predicted = pred.max(1)\n",
    "        acc = (predicted == true).float().mean().item()\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2367a054",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model,\n",
    "    X_train,\n",
    "    y,\n",
    "    train_len,\n",
    "    micro_d,\n",
    "    X_val,\n",
    "    y_val,\n",
    "    micro_d_val,\n",
    "    train_len_val,\n",
    "    device,\n",
    "    config,\n",
    "    learning_rate=5e-5,\n",
    "    epochs=40,\n",
    "    save_path=\"checkpoints\",\n",
    "):\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    model.to(device)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    # optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    scheduler = ReduceLROnPlateau(\n",
    "        optimizer, mode=\"min\", factor=0.5, patience=5, verbose=True\n",
    "    )\n",
    "\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    best_loss = float(\"inf\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_train = y[:, :train_len].to(device)\n",
    "        y_test = y[:, train_len:].to(device)\n",
    "        X_train = X_train.to(device)\n",
    "        micro_d = micro_d.to(device)\n",
    "\n",
    "        pred = model(X_train, y_train, micro_d)  # (B, test_size, max_classes)\n",
    "        pred = pred.flatten(end_dim=-2)\n",
    "        true = y_test.long().flatten()\n",
    "\n",
    "        loss = criterion(pred, true)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step(loss)\n",
    "\n",
    "        _, predicted = pred.max(1)\n",
    "        accuracy = (predicted == true).sum().item() / true.size(0)\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "        train_accuracies.append(accuracy)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}/{epochs}, Loss: {loss.item():.4f}, \"\n",
    "            f\"lr: {optimizer.param_groups[0]['lr']:.6f}, Train Accuracy: {accuracy:.6f}\"\n",
    "        )\n",
    "\n",
    "        # ---- VALIDATION evaluation ----\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_acc = evaluate(model, X_val, y_val, micro_d_val, train_len_val, device)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}/{epochs}, \"\n",
    "            f\"Loss: {loss.item():.4f}, \"\n",
    "            f\"Val Acc: {val_acc:.6f}, \"\n",
    "            f\"lr: {optimizer.param_groups[0]['lr']:.6f}\"\n",
    "        )\n",
    "\n",
    "        # ---- Save best checkpoint ----\n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            checkpoint = {\"config\": config, \"state_dict\": model.state_dict()}\n",
    "            torch.save(checkpoint, os.path.join(save_path, \"model_best.pth\"))\n",
    "            print(f\"Best model saved (loss={best_loss:.4f})\")\n",
    "\n",
    "    # ---- Save final model ----\n",
    "    final_ckpt = {\"config\": config, \"state_dict\": model.state_dict()}\n",
    "    torch.save(final_ckpt, os.path.join(save_path, \"model_final.pth\"))\n",
    "    print(\"Final model saved.\")\n",
    "\n",
    "    return train_losses, train_accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0b2a9060",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/D32485/fmproject/.venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Loss: 0.0109, lr: 0.000050, Train Accuracy: 1.000000\n",
      "Epoch 1/30, Loss: 0.0109, Val Acc: 0.210526, lr: 0.000050\n",
      "Best model saved (loss=0.0109)\n",
      "Epoch 2/30, Loss: 0.5759, lr: 0.000050, Train Accuracy: 0.900000\n",
      "Epoch 2/30, Loss: 0.5759, Val Acc: 0.236842, lr: 0.000050\n",
      "Epoch 3/30, Loss: 0.4007, lr: 0.000050, Train Accuracy: 0.890000\n",
      "Epoch 3/30, Loss: 0.4007, Val Acc: 0.263158, lr: 0.000050\n",
      "Epoch 4/30, Loss: 0.1890, lr: 0.000050, Train Accuracy: 0.940000\n",
      "Epoch 4/30, Loss: 0.1890, Val Acc: 0.263158, lr: 0.000050\n",
      "Epoch 5/30, Loss: 0.2018, lr: 0.000050, Train Accuracy: 0.900000\n",
      "Epoch 5/30, Loss: 0.2018, Val Acc: 0.368421, lr: 0.000050\n",
      "Epoch 6/30, Loss: 0.1978, lr: 0.000050, Train Accuracy: 0.910000\n",
      "Epoch 6/30, Loss: 0.1978, Val Acc: 0.368421, lr: 0.000050\n",
      "Epoch 7/30, Loss: 0.1578, lr: 0.000025, Train Accuracy: 0.940000\n",
      "Epoch 7/30, Loss: 0.1578, Val Acc: 0.421053, lr: 0.000025\n",
      "Epoch 8/30, Loss: 0.1085, lr: 0.000025, Train Accuracy: 0.960000\n",
      "Epoch 8/30, Loss: 0.1085, Val Acc: 0.421053, lr: 0.000025\n",
      "Epoch 9/30, Loss: 0.0882, lr: 0.000025, Train Accuracy: 0.960000\n",
      "Epoch 9/30, Loss: 0.0882, Val Acc: 0.394737, lr: 0.000025\n",
      "Epoch 10/30, Loss: 0.0681, lr: 0.000025, Train Accuracy: 0.990000\n",
      "Epoch 10/30, Loss: 0.0681, Val Acc: 0.368421, lr: 0.000025\n",
      "Epoch 11/30, Loss: 0.0527, lr: 0.000025, Train Accuracy: 1.000000\n",
      "Epoch 11/30, Loss: 0.0527, Val Acc: 0.368421, lr: 0.000025\n",
      "Epoch 12/30, Loss: 0.0415, lr: 0.000025, Train Accuracy: 1.000000\n",
      "Epoch 12/30, Loss: 0.0415, Val Acc: 0.368421, lr: 0.000025\n",
      "Epoch 13/30, Loss: 0.0324, lr: 0.000013, Train Accuracy: 1.000000\n",
      "Epoch 13/30, Loss: 0.0324, Val Acc: 0.368421, lr: 0.000013\n",
      "Epoch 14/30, Loss: 0.0240, lr: 0.000013, Train Accuracy: 1.000000\n",
      "Epoch 14/30, Loss: 0.0240, Val Acc: 0.368421, lr: 0.000013\n",
      "Epoch 15/30, Loss: 0.0201, lr: 0.000013, Train Accuracy: 1.000000\n",
      "Epoch 15/30, Loss: 0.0201, Val Acc: 0.368421, lr: 0.000013\n",
      "Epoch 16/30, Loss: 0.0166, lr: 0.000013, Train Accuracy: 1.000000\n",
      "Epoch 16/30, Loss: 0.0166, Val Acc: 0.368421, lr: 0.000013\n",
      "Epoch 17/30, Loss: 0.0137, lr: 0.000013, Train Accuracy: 1.000000\n",
      "Epoch 17/30, Loss: 0.0137, Val Acc: 0.368421, lr: 0.000013\n",
      "Epoch 18/30, Loss: 0.0114, lr: 0.000013, Train Accuracy: 1.000000\n",
      "Epoch 18/30, Loss: 0.0114, Val Acc: 0.368421, lr: 0.000013\n",
      "Epoch 19/30, Loss: 0.0095, lr: 0.000013, Train Accuracy: 1.000000\n",
      "Epoch 19/30, Loss: 0.0095, Val Acc: 0.368421, lr: 0.000013\n",
      "Best model saved (loss=0.0095)\n",
      "Epoch 20/30, Loss: 0.0079, lr: 0.000013, Train Accuracy: 1.000000\n",
      "Epoch 20/30, Loss: 0.0079, Val Acc: 0.342105, lr: 0.000013\n",
      "Best model saved (loss=0.0079)\n",
      "Epoch 21/30, Loss: 0.0065, lr: 0.000013, Train Accuracy: 1.000000\n",
      "Epoch 21/30, Loss: 0.0065, Val Acc: 0.342105, lr: 0.000013\n",
      "Best model saved (loss=0.0065)\n",
      "Epoch 22/30, Loss: 0.0054, lr: 0.000013, Train Accuracy: 1.000000\n",
      "Epoch 22/30, Loss: 0.0054, Val Acc: 0.342105, lr: 0.000013\n",
      "Best model saved (loss=0.0054)\n",
      "Epoch 23/30, Loss: 0.0046, lr: 0.000013, Train Accuracy: 1.000000\n",
      "Epoch 23/30, Loss: 0.0046, Val Acc: 0.342105, lr: 0.000013\n",
      "Best model saved (loss=0.0046)\n",
      "Epoch 24/30, Loss: 0.0039, lr: 0.000013, Train Accuracy: 1.000000\n",
      "Epoch 24/30, Loss: 0.0039, Val Acc: 0.342105, lr: 0.000013\n",
      "Best model saved (loss=0.0039)\n",
      "Epoch 25/30, Loss: 0.0033, lr: 0.000013, Train Accuracy: 1.000000\n",
      "Epoch 25/30, Loss: 0.0033, Val Acc: 0.342105, lr: 0.000013\n",
      "Best model saved (loss=0.0033)\n",
      "Epoch 26/30, Loss: 0.0029, lr: 0.000013, Train Accuracy: 1.000000\n",
      "Epoch 26/30, Loss: 0.0029, Val Acc: 0.342105, lr: 0.000013\n",
      "Best model saved (loss=0.0029)\n",
      "Epoch 27/30, Loss: 0.0025, lr: 0.000013, Train Accuracy: 1.000000\n",
      "Epoch 27/30, Loss: 0.0025, Val Acc: 0.315789, lr: 0.000013\n",
      "Best model saved (loss=0.0025)\n",
      "Epoch 28/30, Loss: 0.0022, lr: 0.000013, Train Accuracy: 1.000000\n",
      "Epoch 28/30, Loss: 0.0022, Val Acc: 0.315789, lr: 0.000013\n",
      "Best model saved (loss=0.0022)\n",
      "Epoch 29/30, Loss: 0.0020, lr: 0.000013, Train Accuracy: 1.000000\n",
      "Epoch 29/30, Loss: 0.0020, Val Acc: 0.315789, lr: 0.000013\n",
      "Best model saved (loss=0.0020)\n",
      "Epoch 30/30, Loss: 0.0018, lr: 0.000013, Train Accuracy: 1.000000\n",
      "Epoch 30/30, Loss: 0.0018, Val Acc: 0.315789, lr: 0.000013\n",
      "Best model saved (loss=0.0018)\n",
      "Final model saved.\n"
     ]
    }
   ],
   "source": [
    "d = torch.full((batch_size,), H, dtype=torch.long)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "config = {\n",
    "    \"max_classes\": 10,\n",
    "    \"embed_dim\": 128,\n",
    "    \"col_num_blocks\": 3,\n",
    "    \"col_nhead\": 4,\n",
    "    \"col_num_inds\": 128,\n",
    "    \"row_num_blocks\": 3,\n",
    "    \"row_nhead\": 8,\n",
    "    \"row_num_cls\": 4,\n",
    "    \"row_rope_base\": 100000,\n",
    "    \"icl_num_blocks\": 12,\n",
    "    \"icl_nhead\": 4,\n",
    "    \"ff_factor\": 2,\n",
    "    \"dropout\": 0.0,\n",
    "    \"activation\": \"gelu\",\n",
    "    \"norm_first\": True,\n",
    "}\n",
    "### ------------------- Run the full finetuning function ---------------------- ###\n",
    "train_losses, train_accuracies = train(\n",
    "    model=model,\n",
    "    X_train=X_train_tensor,\n",
    "    y=y_train_tensor,\n",
    "    train_len=300,\n",
    "    micro_d=d,\n",
    "    X_val=X_val_tensor,\n",
    "    y_val=y_val_tensor,\n",
    "    micro_d_val=d,\n",
    "    train_len_val=20,\n",
    "    device=device,\n",
    "    config=config,\n",
    "    learning_rate=5e-5,\n",
    "    epochs=30,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7c4b9a",
   "metadata": {},
   "source": [
    "Clearly, we can observe overfitting. Which is not surprising due to the big size of the model and the small amount of data. \n",
    "Another approach would be freezing a major part of the model and fine-tuning only a small part, this could help reduce overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfd46c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Version with mini-batches ##\n",
    "# def train(\n",
    "#     model,\n",
    "#     X_train,\n",
    "#     y,\n",
    "#     train_len,\n",
    "#     micro_d,\n",
    "#     X_val,\n",
    "#     y_val,\n",
    "#     micro_d_val,\n",
    "#     train_len_val,\n",
    "#     device,\n",
    "#     config,\n",
    "#     learning_rate=5e-5,\n",
    "#     epochs=40,\n",
    "#     batch_size=32,\n",
    "#     save_path=\"checkpoints\",\n",
    "# ):\n",
    "#     os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "#     model.to(device)\n",
    "#     criterion = torch.nn.CrossEntropyLoss()\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "#     scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "#     train_dataset = TensorDataset(X_train, y, micro_d)\n",
    "#     train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#     train_losses = []\n",
    "#     train_accuracies = []\n",
    "#     best_loss = float(\"inf\")\n",
    "\n",
    "#     for epoch in range(epochs):\n",
    "#         model.train()\n",
    "#         epoch_loss = 0.0\n",
    "#         correct = 0\n",
    "#         total = 0\n",
    "\n",
    "#         for X_batch, y_batch, micro_batch in train_loader:\n",
    "#             X_batch, y_batch, micro_batch = X_batch.to(device), y_batch.to(device), micro_batch.to(device)\n",
    "#             X_batch = X_batch.unsqueeze(0)\n",
    "#             y_batch = y_batch.unsqueeze(0)\n",
    "#             optimizer.zero_grad()\n",
    "#             # print(y_batch.shape)\n",
    "#             # print(X_batch.shape)\n",
    "#             train_len = 16\n",
    "#             y_train = y_batch[:,:train_len]\n",
    "#             y_test = y_batch[:,train_len:]\n",
    "\n",
    "#             pred = model(X_batch, y_train, micro_batch)\n",
    "#             pred = pred.flatten(end_dim=-2)\n",
    "#             true = y_test.long().flatten()\n",
    "\n",
    "#             loss = criterion(pred, true)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             epoch_loss += loss.item() * X_batch.size(0)\n",
    "#             _, predicted = pred.max(1)\n",
    "#             correct += (predicted == true).sum().item()\n",
    "#             total += true.size(0)\n",
    "\n",
    "#             X_batch = X_batch.squeeze(0)\n",
    "#             y_batch = y_batch.squeeze(0)\n",
    "\n",
    "#         epoch_loss /= len(train_dataset)\n",
    "#         accuracy = correct / total\n",
    "#         train_losses.append(epoch_loss)\n",
    "#         train_accuracies.append(accuracy)\n",
    "\n",
    "#         scheduler.step(epoch_loss)\n",
    "\n",
    "#         print(\n",
    "#             f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss:.4f}, \"\n",
    "#             f\"lr: {optimizer.param_groups[0]['lr']:.6f}, Train Accuracy: {accuracy:.6f}\"\n",
    "#         )\n",
    "\n",
    "#         # ---- VALIDATION evaluation ----\n",
    "#         model.eval()\n",
    "#         with torch.no_grad():\n",
    "#             X_val = X_val.unsqueeze(0)\n",
    "#             y_val = y_val.unsqueeze(0)\n",
    "#             print(y_val.shape)\n",
    "#             val_acc = evaluate(model, X_val, y_val, micro_d_val, train_len_val, device)\n",
    "#             X_val = X_val.squeeze(0)\n",
    "#             y_val = y_val.squeeze(0)\n",
    "\n",
    "#         print(\n",
    "#             f\"Epoch {epoch + 1}/{epochs}, \"\n",
    "#             f\"Loss: {epoch_loss:.4f}, \"\n",
    "#             f\"Val Acc: {val_acc:.6f}, \"\n",
    "#             f\"lr: {optimizer.param_groups[0]['lr']:.6f}\"\n",
    "#         )\n",
    "\n",
    "#         # ---- Save best checkpoint ----\n",
    "#         if epoch_loss < best_loss:\n",
    "#             best_loss = epoch_loss\n",
    "#             checkpoint = {\"config\": config, \"state_dict\": model.state_dict()}\n",
    "#             torch.save(checkpoint, os.path.join(save_path, \"model_best.pth\"))\n",
    "#             print(f\"Best model saved (loss={best_loss:.4f})\")\n",
    "\n",
    "#     # ---- Save final model ----\n",
    "#     final_ckpt = {\"config\": config, \"state_dict\": model.state_dict()}\n",
    "#     torch.save(final_ckpt, os.path.join(save_path, \"model_final.pth\"))\n",
    "#     print(\"Final model saved.\")\n",
    "\n",
    "#     return train_losses, train_accuracies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9ce965",
   "metadata": {},
   "source": [
    "### Inference\n",
    "The classifier expects Data Frames as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00274394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load checkpoints custom !\n",
      "Original checkpoint !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/D32485/exercice/tabicl/src/tabicl/sklearn/classifier.py:258: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Accuracy: 0.5400\n"
     ]
    }
   ],
   "source": [
    "clf_original = TabICLClassifier(checkpoint_version=\"original\")\n",
    "clf_original.fit(X_val, y_val)  \n",
    "y_pred_original = clf_original.predict(X_test)  # in-context learning happens here\n",
    "# Compute accuracy\n",
    "acc = accuracy_score(y_test, y_pred_original)\n",
    "print(f\"Baseline Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00af6d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load checkpoints custom !\n",
      "Original checkpoint !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/D32485/exercice/tabicl/src/tabicl/sklearn/classifier.py:258: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Accuracy: 0.6800\n"
     ]
    }
   ],
   "source": [
    "clf_original = TabICLClassifier(checkpoint_version=\"original\")\n",
    "clf_original.fit(X_train, y_train)  \n",
    "y_pred_original = clf_original.predict(X_test)  # in-context learning happens here\n",
    "# Compute accuracy\n",
    "acc = accuracy_score(y_test, y_pred_original)\n",
    "print(f\"Baseline Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d952e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load checkpoints custom !\n",
      "Fine-tuned checkpoint !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/D32485/exercice/tabicl/src/tabicl/sklearn/classifier.py:258: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetuned Accuracy: 0.5800\n"
     ]
    }
   ],
   "source": [
    "clf_finetuned = TabICLClassifier()\n",
    "clf_finetuned.fit(X_val, y_val) \n",
    "y_pred_finetuned = clf_finetuned.predict(X_test)  # in-context learning happens here\n",
    "# Compute accuracy\n",
    "acc = accuracy_score(y_test, y_pred_finetuned)\n",
    "print(f\"Finetuned Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16a910a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load checkpoints custom !\n",
      "Fine-tuned checkpoint !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/D32485/exercice/tabicl/src/tabicl/sklearn/classifier.py:258: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetuned Accuracy: 0.6200\n"
     ]
    }
   ],
   "source": [
    "clf_finetuned = TabICLClassifier()\n",
    "clf_finetuned.fit(X_train, y_train) \n",
    "y_pred_finetuned = clf_finetuned.predict(X_test)  # in-context learning happens here\n",
    "# Compute accuracy\n",
    "acc = accuracy_score(y_test, y_pred_finetuned)\n",
    "print(f\"Finetuned Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab214bf",
   "metadata": {},
   "source": [
    "### LoRA finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a70b8cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/D32485/exercice/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71332e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80749daf",
   "metadata": {},
   "source": [
    "I want to apply LoRA to linear layers inside of the attention blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f73f648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn1.linear1\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn1.linear1.base_layer\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn1.linear1.lora_dropout\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn1.linear1.lora_dropout.default\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn1.linear1.lora_A\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn1.linear1.lora_A.default\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn1.linear1.lora_B\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn1.linear1.lora_B.default\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn1.linear1.lora_embedding_A\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn1.linear1.lora_embedding_B\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn1.linear1.lora_magnitude_vector\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn1.linear2\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn1.linear2.base_layer\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn1.linear2.lora_dropout\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn1.linear2.lora_dropout.default\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn1.linear2.lora_A\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn1.linear2.lora_A.default\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn1.linear2.lora_B\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn1.linear2.lora_B.default\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn1.linear2.lora_embedding_A\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn1.linear2.lora_embedding_B\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn1.linear2.lora_magnitude_vector\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn1.attn.out_proj\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn1.attn.out_proj.base_layer\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn1.attn.out_proj.lora_dropout\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn1.attn.out_proj.lora_dropout.default\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn1.attn.out_proj.lora_A\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn1.attn.out_proj.lora_A.default\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn1.attn.out_proj.lora_B\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn1.attn.out_proj.lora_B.default\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn1.attn.out_proj.lora_embedding_A\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn1.attn.out_proj.lora_embedding_B\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn1.attn.out_proj.lora_magnitude_vector\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn2.linear1\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn2.linear1.base_layer\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn2.linear1.lora_dropout\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn2.linear1.lora_dropout.default\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn2.linear1.lora_A\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn2.linear1.lora_A.default\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn2.linear1.lora_B\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn2.linear1.lora_B.default\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn2.linear1.lora_embedding_A\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn2.linear1.lora_embedding_B\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn2.linear1.lora_magnitude_vector\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn2.linear2\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn2.linear2.base_layer\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn2.linear2.lora_dropout\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn2.linear2.lora_dropout.default\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn2.linear2.lora_A\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn2.linear2.lora_A.default\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn2.linear2.lora_B\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn2.linear2.lora_B.default\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn2.linear2.lora_embedding_A\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn2.linear2.lora_embedding_B\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn2.linear2.lora_magnitude_vector\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn2.attn.out_proj\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn2.attn.out_proj.base_layer\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn2.attn.out_proj.lora_dropout\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn2.attn.out_proj.lora_dropout.default\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn2.attn.out_proj.lora_A\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn2.attn.out_proj.lora_A.default\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn2.attn.out_proj.lora_B\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn2.attn.out_proj.lora_B.default\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn2.attn.out_proj.lora_embedding_A\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn2.attn.out_proj.lora_embedding_B\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn2.attn.out_proj.lora_magnitude_vector\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn1.linear1\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn1.linear1.base_layer\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn1.linear1.lora_dropout\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn1.linear1.lora_dropout.default\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn1.linear1.lora_A\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn1.linear1.lora_A.default\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn1.linear1.lora_B\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn1.linear1.lora_B.default\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn1.linear1.lora_embedding_A\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn1.linear1.lora_embedding_B\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn1.linear1.lora_magnitude_vector\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn1.linear2\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn1.linear2.base_layer\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn1.linear2.lora_dropout\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn1.linear2.lora_dropout.default\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn1.linear2.lora_A\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn1.linear2.lora_A.default\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn1.linear2.lora_B\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn1.linear2.lora_B.default\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn1.linear2.lora_embedding_A\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn1.linear2.lora_embedding_B\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn1.linear2.lora_magnitude_vector\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn1.attn.out_proj\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn1.attn.out_proj.base_layer\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn1.attn.out_proj.lora_dropout\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn1.attn.out_proj.lora_dropout.default\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn1.attn.out_proj.lora_A\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn1.attn.out_proj.lora_A.default\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn1.attn.out_proj.lora_B\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn1.attn.out_proj.lora_B.default\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn1.attn.out_proj.lora_embedding_A\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn1.attn.out_proj.lora_embedding_B\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn1.attn.out_proj.lora_magnitude_vector\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn2.linear1\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn2.linear1.base_layer\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn2.linear1.lora_dropout\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn2.linear1.lora_dropout.default\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn2.linear1.lora_A\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn2.linear1.lora_A.default\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn2.linear1.lora_B\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn2.linear1.lora_B.default\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn2.linear1.lora_embedding_A\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn2.linear1.lora_embedding_B\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn2.linear1.lora_magnitude_vector\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn2.linear2\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn2.linear2.base_layer\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn2.linear2.lora_dropout\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn2.linear2.lora_dropout.default\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn2.linear2.lora_A\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn2.linear2.lora_A.default\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn2.linear2.lora_B\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn2.linear2.lora_B.default\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn2.linear2.lora_embedding_A\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn2.linear2.lora_embedding_B\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn2.linear2.lora_magnitude_vector\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn2.attn.out_proj\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn2.attn.out_proj.base_layer\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn2.attn.out_proj.lora_dropout\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn2.attn.out_proj.lora_dropout.default\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn2.attn.out_proj.lora_A\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn2.attn.out_proj.lora_A.default\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn2.attn.out_proj.lora_B\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn2.attn.out_proj.lora_B.default\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn2.attn.out_proj.lora_embedding_A\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn2.attn.out_proj.lora_embedding_B\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn2.attn.out_proj.lora_magnitude_vector\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn1.linear1\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn1.linear1.base_layer\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn1.linear1.lora_dropout\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn1.linear1.lora_dropout.default\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn1.linear1.lora_A\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn1.linear1.lora_A.default\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn1.linear1.lora_B\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn1.linear1.lora_B.default\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn1.linear1.lora_embedding_A\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn1.linear1.lora_embedding_B\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn1.linear1.lora_magnitude_vector\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn1.linear2\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn1.linear2.base_layer\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn1.linear2.lora_dropout\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn1.linear2.lora_dropout.default\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn1.linear2.lora_A\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn1.linear2.lora_A.default\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn1.linear2.lora_B\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn1.linear2.lora_B.default\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn1.linear2.lora_embedding_A\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn1.linear2.lora_embedding_B\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn1.linear2.lora_magnitude_vector\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn1.attn.out_proj\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn1.attn.out_proj.base_layer\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn1.attn.out_proj.lora_dropout\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn1.attn.out_proj.lora_dropout.default\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn1.attn.out_proj.lora_A\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn1.attn.out_proj.lora_A.default\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn1.attn.out_proj.lora_B\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn1.attn.out_proj.lora_B.default\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn1.attn.out_proj.lora_embedding_A\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn1.attn.out_proj.lora_embedding_B\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn1.attn.out_proj.lora_magnitude_vector\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn2.linear1\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn2.linear1.base_layer\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn2.linear1.lora_dropout\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn2.linear1.lora_dropout.default\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn2.linear1.lora_A\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn2.linear1.lora_A.default\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn2.linear1.lora_B\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn2.linear1.lora_B.default\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn2.linear1.lora_embedding_A\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn2.linear1.lora_embedding_B\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn2.linear1.lora_magnitude_vector\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn2.linear2\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn2.linear2.base_layer\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn2.linear2.lora_dropout\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn2.linear2.lora_dropout.default\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn2.linear2.lora_A\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn2.linear2.lora_A.default\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn2.linear2.lora_B\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn2.linear2.lora_B.default\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn2.linear2.lora_embedding_A\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn2.linear2.lora_embedding_B\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn2.linear2.lora_magnitude_vector\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn2.attn.out_proj\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn2.attn.out_proj.base_layer\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn2.attn.out_proj.lora_dropout\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn2.attn.out_proj.lora_dropout.default\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn2.attn.out_proj.lora_A\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn2.attn.out_proj.lora_A.default\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn2.attn.out_proj.lora_B\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn2.attn.out_proj.lora_B.default\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn2.attn.out_proj.lora_embedding_A\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn2.attn.out_proj.lora_embedding_B\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn2.attn.out_proj.lora_magnitude_vector\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/D32485/exercice/.venv/lib/python3.12/site-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/home/D32485/exercice/.venv/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"linear1\", \"linear2\", \"attn.out_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\",\n",
    ")\n",
    "\n",
    "model.col_embedder = get_peft_model(model.col_embedder, lora_config)\n",
    "\n",
    "for name, module in model.col_embedder.named_modules():\n",
    "    if any(target in name for target in lora_config.target_modules):\n",
    "        print(\"LoRA will be applied to in column embedder:\", name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4958b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vae",
   "language": "python",
   "name": "vae"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
