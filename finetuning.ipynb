{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab441572",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36f05abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import os\n",
    "\n",
    "import arff\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from typing import Optional, List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba618e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/D32485/exercice/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add current folder to sys.path so Python sees \"tabicl\"\n",
    "sys.path.insert(0, os.path.abspath(\".\"))\n",
    "\n",
    "from tabicl.src.tabicl.model.tabicl import TabICL\n",
    "from tabicl.src.tabicl.sklearn.classifier import TabICLClassifier\n",
    "from tabicl.src.tabicl.sklearn.preprocessing import (\n",
    "    TransformToNumerical,\n",
    "    EnsembleGenerator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e28856b",
   "metadata": {},
   "source": [
    "### Load and process dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0fadd36",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'generator' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata.arff\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      2\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m arff\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m----> 3\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, columns\u001b[38;5;241m=\u001b[39m[attr[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattributes\u001b[39m\u001b[38;5;124m\"\u001b[39m]])\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mhead())\n",
      "\u001b[0;31mTypeError\u001b[0m: 'generator' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "with open(\"data.arff\") as f:\n",
    "    dataset = arff.load(f)\n",
    "df = pd.DataFrame(dataset[\"data\"], columns=[attr[0] for attr in dataset[\"attributes\"]])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce3599da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 500\n",
      "Number of columns: 13\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of rows: {df.shape[0]}\")\n",
    "print(f\"Number of columns: {df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a50483de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (400, 12) (400,)\n",
      "Val:   (50, 12) (50,)\n",
      "Test:  (50, 12) (50,)\n",
      "Classes:  ['2' '1']\n"
     ]
    }
   ],
   "source": [
    "# Separate features and target\n",
    "X = df.drop(columns=[\"Class\"])\n",
    "y = df[\"Class\"]\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(\"Train:\", X_train.shape, y_train.shape)\n",
    "print(\"Val:  \", X_val.shape, y_val.shape)\n",
    "print(\"Test: \", X_test.shape, y_test.shape)\n",
    "\n",
    "print(\"Classes: \", y.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aca1bc79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>EnsembleGenerator(class_shift=(True,), n_estimators=32,\n",
       "                  norm_methods=[&#x27;none&#x27;, &#x27;power&#x27;], outlier_threshold=(4.0,))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;EnsembleGenerator<span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>EnsembleGenerator(class_shift=(True,), n_estimators=32,\n",
       "                  norm_methods=[&#x27;none&#x27;, &#x27;power&#x27;], outlier_threshold=(4.0,))</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "EnsembleGenerator(class_shift=(True,), n_estimators=32,\n",
       "                  norm_methods=['none', 'power'], outlier_threshold=(4.0,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_encoder_ = LabelEncoder()\n",
    "y_train = y_encoder_.fit_transform(y_train)\n",
    "classes_ = y_encoder_.classes_\n",
    "n_classes_ = len(y_encoder_.classes_)\n",
    "\n",
    "\n",
    "#  Transform input features\n",
    "X_encoder_ = TransformToNumerical(verbose=False)\n",
    "X_train = X_encoder_.fit_transform(X_train)\n",
    "\n",
    "n_estimators: int = (32,)\n",
    "norm_methods: Optional[str | List[str]] = (None,)\n",
    "feat_shuffle_method: str = (\"latin\",)\n",
    "class_shift: bool = (True,)\n",
    "outlier_threshold: float = (4.0,)\n",
    "softmax_temperature: float = (0.9,)\n",
    "average_logits: bool = (True,)\n",
    "use_hierarchical: bool = True\n",
    "random_state: int | None = (42,)\n",
    "\n",
    "seed = random_state if isinstance(random_state, (int, type(None))) else None\n",
    "\n",
    "# Fit ensemble generator to create multiple dataset views\n",
    "ensemble_generator_ = EnsembleGenerator(\n",
    "    n_estimators=32,\n",
    "    norm_methods=[\"none\", \"power\"],\n",
    "    feat_shuffle_method=\"latin\",\n",
    "    class_shift=class_shift,\n",
    "    outlier_threshold=outlier_threshold,\n",
    "    random_state=seed,\n",
    ")\n",
    "ensemble_generator_.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9483023a",
   "metadata": {},
   "source": [
    "### Split data into Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95203ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     X, y, test_size=0.2, random_state=seed, stratify=y\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7be91ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train tensor shape:  torch.Size([1, 400, 12])\n",
      "y train tensor shape:  torch.Size([1, 400])\n"
     ]
    }
   ],
   "source": [
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "# X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "# y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "X_train_tensor = X_train_tensor.unsqueeze(0)\n",
    "y_train_tensor = y_train_tensor.unsqueeze(0)\n",
    "\n",
    "print(\"X train tensor shape: \", X_train_tensor.shape)\n",
    "print(\"y train tensor shape: \", y_train_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95d0be4",
   "metadata": {},
   "source": [
    "### Load model and weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f492c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters:  27051666\n"
     ]
    }
   ],
   "source": [
    "model = TabICL()\n",
    "\n",
    "\n",
    "def cnt_params(model):\n",
    "    return sum(param.numel() for param in model.parameters())\n",
    "\n",
    "\n",
    "print(\"Number of parameters: \", cnt_params(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdef7b57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ColEmbedding(\n",
       "  (in_linear): SkippableLinear(in_features=1, out_features=128, bias=True)\n",
       "  (tf_col): SetTransformer(\n",
       "    (blocks): ModuleList(\n",
       "      (0-2): 3 x InducedSelfAttentionBlock(\n",
       "        (multihead_attn1): MultiheadAttentionBlock(\n",
       "          (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (linear2): Linear(in_features=256, out_features=128, bias=True)\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.0, inplace=False)\n",
       "          (dropout2): Dropout(p=0.0, inplace=False)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (multihead_attn2): MultiheadAttentionBlock(\n",
       "          (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (linear2): Linear(in_features=256, out_features=128, bias=True)\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.0, inplace=False)\n",
       "          (dropout2): Dropout(p=0.0, inplace=False)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (out_w): SkippableLinear(in_features=128, out_features=128, bias=True)\n",
       "  (ln_w): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (out_b): SkippableLinear(in_features=128, out_features=128, bias=True)\n",
       "  (ln_b): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.col_embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94bcb358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module:  | Params: 27051658\n",
      "Module: col_embedder | Params: 877824\n",
      "Module: col_embedder.in_linear | Params: 256\n",
      "Module: col_embedder.tf_col | Params: 844032\n",
      "Module: col_embedder.tf_col.blocks | Params: 844032\n",
      "Module: col_embedder.tf_col.blocks.0 | Params: 281344\n",
      "Module: col_embedder.tf_col.blocks.0.multihead_attn1 | Params: 132480\n",
      "Module: col_embedder.tf_col.blocks.0.multihead_attn1.linear1 | Params: 33024\n",
      "Module: col_embedder.tf_col.blocks.0.multihead_attn1.linear2 | Params: 32896\n",
      "Module: col_embedder.tf_col.blocks.0.multihead_attn1.norm1 | Params: 256\n",
      "Module: col_embedder.tf_col.blocks.0.multihead_attn1.norm2 | Params: 256\n",
      "Module: col_embedder.tf_col.blocks.0.multihead_attn1.attn | Params: 66048\n",
      "Module: col_embedder.tf_col.blocks.0.multihead_attn1.attn.out_proj | Params: 16512\n",
      "Module: col_embedder.tf_col.blocks.0.multihead_attn2 | Params: 132480\n",
      "Module: col_embedder.tf_col.blocks.0.multihead_attn2.linear1 | Params: 33024\n",
      "Module: col_embedder.tf_col.blocks.0.multihead_attn2.linear2 | Params: 32896\n",
      "Module: col_embedder.tf_col.blocks.0.multihead_attn2.norm1 | Params: 256\n",
      "Module: col_embedder.tf_col.blocks.0.multihead_attn2.norm2 | Params: 256\n",
      "Module: col_embedder.tf_col.blocks.0.multihead_attn2.attn | Params: 66048\n",
      "Module: col_embedder.tf_col.blocks.0.multihead_attn2.attn.out_proj | Params: 16512\n",
      "Module: col_embedder.tf_col.blocks.1 | Params: 281344\n",
      "Module: col_embedder.tf_col.blocks.1.multihead_attn1 | Params: 132480\n",
      "Module: col_embedder.tf_col.blocks.1.multihead_attn1.linear1 | Params: 33024\n",
      "Module: col_embedder.tf_col.blocks.1.multihead_attn1.linear2 | Params: 32896\n",
      "Module: col_embedder.tf_col.blocks.1.multihead_attn1.norm1 | Params: 256\n",
      "Module: col_embedder.tf_col.blocks.1.multihead_attn1.norm2 | Params: 256\n",
      "Module: col_embedder.tf_col.blocks.1.multihead_attn1.attn | Params: 66048\n",
      "Module: col_embedder.tf_col.blocks.1.multihead_attn1.attn.out_proj | Params: 16512\n",
      "Module: col_embedder.tf_col.blocks.1.multihead_attn2 | Params: 132480\n",
      "Module: col_embedder.tf_col.blocks.1.multihead_attn2.linear1 | Params: 33024\n",
      "Module: col_embedder.tf_col.blocks.1.multihead_attn2.linear2 | Params: 32896\n",
      "Module: col_embedder.tf_col.blocks.1.multihead_attn2.norm1 | Params: 256\n",
      "Module: col_embedder.tf_col.blocks.1.multihead_attn2.norm2 | Params: 256\n",
      "Module: col_embedder.tf_col.blocks.1.multihead_attn2.attn | Params: 66048\n",
      "Module: col_embedder.tf_col.blocks.1.multihead_attn2.attn.out_proj | Params: 16512\n",
      "Module: col_embedder.tf_col.blocks.2 | Params: 281344\n",
      "Module: col_embedder.tf_col.blocks.2.multihead_attn1 | Params: 132480\n",
      "Module: col_embedder.tf_col.blocks.2.multihead_attn1.linear1 | Params: 33024\n",
      "Module: col_embedder.tf_col.blocks.2.multihead_attn1.linear2 | Params: 32896\n",
      "Module: col_embedder.tf_col.blocks.2.multihead_attn1.norm1 | Params: 256\n",
      "Module: col_embedder.tf_col.blocks.2.multihead_attn1.norm2 | Params: 256\n",
      "Module: col_embedder.tf_col.blocks.2.multihead_attn1.attn | Params: 66048\n",
      "Module: col_embedder.tf_col.blocks.2.multihead_attn1.attn.out_proj | Params: 16512\n",
      "Module: col_embedder.tf_col.blocks.2.multihead_attn2 | Params: 132480\n",
      "Module: col_embedder.tf_col.blocks.2.multihead_attn2.linear1 | Params: 33024\n",
      "Module: col_embedder.tf_col.blocks.2.multihead_attn2.linear2 | Params: 32896\n",
      "Module: col_embedder.tf_col.blocks.2.multihead_attn2.norm1 | Params: 256\n",
      "Module: col_embedder.tf_col.blocks.2.multihead_attn2.norm2 | Params: 256\n",
      "Module: col_embedder.tf_col.blocks.2.multihead_attn2.attn | Params: 66048\n",
      "Module: col_embedder.tf_col.blocks.2.multihead_attn2.attn.out_proj | Params: 16512\n",
      "Module: col_embedder.out_w | Params: 16512\n",
      "Module: col_embedder.ln_w | Params: 256\n",
      "Module: col_embedder.out_b | Params: 16512\n",
      "Module: col_embedder.ln_b | Params: 256\n",
      "Module: row_interactor | Params: 398208\n",
      "Module: row_interactor.tf_row | Params: 397440\n",
      "Module: row_interactor.tf_row.blocks | Params: 397440\n",
      "Module: row_interactor.tf_row.blocks.0 | Params: 132480\n",
      "Module: row_interactor.tf_row.blocks.0.linear1 | Params: 33024\n",
      "Module: row_interactor.tf_row.blocks.0.linear2 | Params: 32896\n",
      "Module: row_interactor.tf_row.blocks.0.norm1 | Params: 256\n",
      "Module: row_interactor.tf_row.blocks.0.norm2 | Params: 256\n",
      "Module: row_interactor.tf_row.blocks.0.attn | Params: 66048\n",
      "Module: row_interactor.tf_row.blocks.0.attn.out_proj | Params: 16512\n",
      "Module: row_interactor.tf_row.blocks.1 | Params: 132480\n",
      "Module: row_interactor.tf_row.blocks.1.linear1 | Params: 33024\n",
      "Module: row_interactor.tf_row.blocks.1.linear2 | Params: 32896\n",
      "Module: row_interactor.tf_row.blocks.1.norm1 | Params: 256\n",
      "Module: row_interactor.tf_row.blocks.1.norm2 | Params: 256\n",
      "Module: row_interactor.tf_row.blocks.1.attn | Params: 66048\n",
      "Module: row_interactor.tf_row.blocks.1.attn.out_proj | Params: 16512\n",
      "Module: row_interactor.tf_row.blocks.2 | Params: 132480\n",
      "Module: row_interactor.tf_row.blocks.2.linear1 | Params: 33024\n",
      "Module: row_interactor.tf_row.blocks.2.linear2 | Params: 32896\n",
      "Module: row_interactor.tf_row.blocks.2.norm1 | Params: 256\n",
      "Module: row_interactor.tf_row.blocks.2.norm2 | Params: 256\n",
      "Module: row_interactor.tf_row.blocks.2.attn | Params: 66048\n",
      "Module: row_interactor.tf_row.blocks.2.attn.out_proj | Params: 16512\n",
      "Module: row_interactor.out_ln | Params: 256\n",
      "Module: icl_predictor | Params: 25775626\n",
      "Module: icl_predictor.tf_icl | Params: 25233408\n",
      "Module: icl_predictor.tf_icl.blocks | Params: 25233408\n",
      "Module: icl_predictor.tf_icl.blocks.0 | Params: 2102784\n",
      "Module: icl_predictor.tf_icl.blocks.0.linear1 | Params: 525312\n",
      "Module: icl_predictor.tf_icl.blocks.0.linear2 | Params: 524800\n",
      "Module: icl_predictor.tf_icl.blocks.0.norm1 | Params: 1024\n",
      "Module: icl_predictor.tf_icl.blocks.0.norm2 | Params: 1024\n",
      "Module: icl_predictor.tf_icl.blocks.0.attn | Params: 1050624\n",
      "Module: icl_predictor.tf_icl.blocks.0.attn.out_proj | Params: 262656\n",
      "Module: icl_predictor.tf_icl.blocks.1 | Params: 2102784\n",
      "Module: icl_predictor.tf_icl.blocks.1.linear1 | Params: 525312\n",
      "Module: icl_predictor.tf_icl.blocks.1.linear2 | Params: 524800\n",
      "Module: icl_predictor.tf_icl.blocks.1.norm1 | Params: 1024\n",
      "Module: icl_predictor.tf_icl.blocks.1.norm2 | Params: 1024\n",
      "Module: icl_predictor.tf_icl.blocks.1.attn | Params: 1050624\n",
      "Module: icl_predictor.tf_icl.blocks.1.attn.out_proj | Params: 262656\n",
      "Module: icl_predictor.tf_icl.blocks.2 | Params: 2102784\n",
      "Module: icl_predictor.tf_icl.blocks.2.linear1 | Params: 525312\n",
      "Module: icl_predictor.tf_icl.blocks.2.linear2 | Params: 524800\n",
      "Module: icl_predictor.tf_icl.blocks.2.norm1 | Params: 1024\n",
      "Module: icl_predictor.tf_icl.blocks.2.norm2 | Params: 1024\n",
      "Module: icl_predictor.tf_icl.blocks.2.attn | Params: 1050624\n",
      "Module: icl_predictor.tf_icl.blocks.2.attn.out_proj | Params: 262656\n",
      "Module: icl_predictor.tf_icl.blocks.3 | Params: 2102784\n",
      "Module: icl_predictor.tf_icl.blocks.3.linear1 | Params: 525312\n",
      "Module: icl_predictor.tf_icl.blocks.3.linear2 | Params: 524800\n",
      "Module: icl_predictor.tf_icl.blocks.3.norm1 | Params: 1024\n",
      "Module: icl_predictor.tf_icl.blocks.3.norm2 | Params: 1024\n",
      "Module: icl_predictor.tf_icl.blocks.3.attn | Params: 1050624\n",
      "Module: icl_predictor.tf_icl.blocks.3.attn.out_proj | Params: 262656\n",
      "Module: icl_predictor.tf_icl.blocks.4 | Params: 2102784\n",
      "Module: icl_predictor.tf_icl.blocks.4.linear1 | Params: 525312\n",
      "Module: icl_predictor.tf_icl.blocks.4.linear2 | Params: 524800\n",
      "Module: icl_predictor.tf_icl.blocks.4.norm1 | Params: 1024\n",
      "Module: icl_predictor.tf_icl.blocks.4.norm2 | Params: 1024\n",
      "Module: icl_predictor.tf_icl.blocks.4.attn | Params: 1050624\n",
      "Module: icl_predictor.tf_icl.blocks.4.attn.out_proj | Params: 262656\n",
      "Module: icl_predictor.tf_icl.blocks.5 | Params: 2102784\n",
      "Module: icl_predictor.tf_icl.blocks.5.linear1 | Params: 525312\n",
      "Module: icl_predictor.tf_icl.blocks.5.linear2 | Params: 524800\n",
      "Module: icl_predictor.tf_icl.blocks.5.norm1 | Params: 1024\n",
      "Module: icl_predictor.tf_icl.blocks.5.norm2 | Params: 1024\n",
      "Module: icl_predictor.tf_icl.blocks.5.attn | Params: 1050624\n",
      "Module: icl_predictor.tf_icl.blocks.5.attn.out_proj | Params: 262656\n",
      "Module: icl_predictor.tf_icl.blocks.6 | Params: 2102784\n",
      "Module: icl_predictor.tf_icl.blocks.6.linear1 | Params: 525312\n",
      "Module: icl_predictor.tf_icl.blocks.6.linear2 | Params: 524800\n",
      "Module: icl_predictor.tf_icl.blocks.6.norm1 | Params: 1024\n",
      "Module: icl_predictor.tf_icl.blocks.6.norm2 | Params: 1024\n",
      "Module: icl_predictor.tf_icl.blocks.6.attn | Params: 1050624\n",
      "Module: icl_predictor.tf_icl.blocks.6.attn.out_proj | Params: 262656\n",
      "Module: icl_predictor.tf_icl.blocks.7 | Params: 2102784\n",
      "Module: icl_predictor.tf_icl.blocks.7.linear1 | Params: 525312\n",
      "Module: icl_predictor.tf_icl.blocks.7.linear2 | Params: 524800\n",
      "Module: icl_predictor.tf_icl.blocks.7.norm1 | Params: 1024\n",
      "Module: icl_predictor.tf_icl.blocks.7.norm2 | Params: 1024\n",
      "Module: icl_predictor.tf_icl.blocks.7.attn | Params: 1050624\n",
      "Module: icl_predictor.tf_icl.blocks.7.attn.out_proj | Params: 262656\n",
      "Module: icl_predictor.tf_icl.blocks.8 | Params: 2102784\n",
      "Module: icl_predictor.tf_icl.blocks.8.linear1 | Params: 525312\n",
      "Module: icl_predictor.tf_icl.blocks.8.linear2 | Params: 524800\n",
      "Module: icl_predictor.tf_icl.blocks.8.norm1 | Params: 1024\n",
      "Module: icl_predictor.tf_icl.blocks.8.norm2 | Params: 1024\n",
      "Module: icl_predictor.tf_icl.blocks.8.attn | Params: 1050624\n",
      "Module: icl_predictor.tf_icl.blocks.8.attn.out_proj | Params: 262656\n",
      "Module: icl_predictor.tf_icl.blocks.9 | Params: 2102784\n",
      "Module: icl_predictor.tf_icl.blocks.9.linear1 | Params: 525312\n",
      "Module: icl_predictor.tf_icl.blocks.9.linear2 | Params: 524800\n",
      "Module: icl_predictor.tf_icl.blocks.9.norm1 | Params: 1024\n",
      "Module: icl_predictor.tf_icl.blocks.9.norm2 | Params: 1024\n",
      "Module: icl_predictor.tf_icl.blocks.9.attn | Params: 1050624\n",
      "Module: icl_predictor.tf_icl.blocks.9.attn.out_proj | Params: 262656\n",
      "Module: icl_predictor.tf_icl.blocks.10 | Params: 2102784\n",
      "Module: icl_predictor.tf_icl.blocks.10.linear1 | Params: 525312\n",
      "Module: icl_predictor.tf_icl.blocks.10.linear2 | Params: 524800\n",
      "Module: icl_predictor.tf_icl.blocks.10.norm1 | Params: 1024\n",
      "Module: icl_predictor.tf_icl.blocks.10.norm2 | Params: 1024\n",
      "Module: icl_predictor.tf_icl.blocks.10.attn | Params: 1050624\n",
      "Module: icl_predictor.tf_icl.blocks.10.attn.out_proj | Params: 262656\n",
      "Module: icl_predictor.tf_icl.blocks.11 | Params: 2102784\n",
      "Module: icl_predictor.tf_icl.blocks.11.linear1 | Params: 525312\n",
      "Module: icl_predictor.tf_icl.blocks.11.linear2 | Params: 524800\n",
      "Module: icl_predictor.tf_icl.blocks.11.norm1 | Params: 1024\n",
      "Module: icl_predictor.tf_icl.blocks.11.norm2 | Params: 1024\n",
      "Module: icl_predictor.tf_icl.blocks.11.attn | Params: 1050624\n",
      "Module: icl_predictor.tf_icl.blocks.11.attn.out_proj | Params: 262656\n",
      "Module: icl_predictor.ln | Params: 1024\n",
      "Module: icl_predictor.y_encoder | Params: 5632\n",
      "Module: icl_predictor.decoder | Params: 535562\n",
      "Module: icl_predictor.decoder.0 | Params: 525312\n",
      "Module: icl_predictor.decoder.2 | Params: 10250\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    params = sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "    if params > 0:\n",
    "        print(f\"Module: {name} | Params: {params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c3564fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32231/3814295166.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TabICL(\n",
       "  (col_embedder): ColEmbedding(\n",
       "    (in_linear): SkippableLinear(in_features=1, out_features=128, bias=True)\n",
       "    (tf_col): SetTransformer(\n",
       "      (blocks): ModuleList(\n",
       "        (0-2): 3 x InducedSelfAttentionBlock(\n",
       "          (multihead_attn1): MultiheadAttentionBlock(\n",
       "            (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (linear2): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.0, inplace=False)\n",
       "            (dropout2): Dropout(p=0.0, inplace=False)\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (multihead_attn2): MultiheadAttentionBlock(\n",
       "            (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (linear2): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.0, inplace=False)\n",
       "            (dropout2): Dropout(p=0.0, inplace=False)\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (out_w): SkippableLinear(in_features=128, out_features=128, bias=True)\n",
       "    (ln_w): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (out_b): SkippableLinear(in_features=128, out_features=128, bias=True)\n",
       "    (ln_b): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (row_interactor): RowInteraction(\n",
       "    (tf_row): Encoder(\n",
       "      (blocks): ModuleList(\n",
       "        (0-2): 3 x MultiheadAttentionBlock(\n",
       "          (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (linear2): Linear(in_features=256, out_features=128, bias=True)\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.0, inplace=False)\n",
       "          (dropout2): Dropout(p=0.0, inplace=False)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (rope): RotaryEmbedding()\n",
       "    )\n",
       "    (out_ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (icl_predictor): ICLearning(\n",
       "    (tf_icl): Encoder(\n",
       "      (blocks): ModuleList(\n",
       "        (0-11): 12 x MultiheadAttentionBlock(\n",
       "          (linear1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (linear2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.0, inplace=False)\n",
       "          (dropout2): Dropout(p=0.0, inplace=False)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (y_encoder): OneHotAndLinear(in_features=10, out_features=512, bias=True)\n",
       "    (decoder): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Linear(in_features=1024, out_features=10, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "checkpoint = torch.load(\n",
    "    \"/home/D32485/exercice/tabicl-classifier-v1.1-0506.ckpt\", map_location=\"cpu\"\n",
    ")\n",
    "model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f46234",
   "metadata": {},
   "source": [
    "### Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "11a906f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size:  1\n",
      "seq_len :  400\n",
      "train_size :  300\n"
     ]
    }
   ],
   "source": [
    "batch_size, T, H = X_train_tensor.shape\n",
    "d = torch.full((batch_size,), H, dtype=torch.long)\n",
    "seq_len = T\n",
    "train_size = 300\n",
    "\n",
    "print(\"Batch size: \", batch_size)\n",
    "print(\"seq_len : \", seq_len)\n",
    "print(\"train_size : \", train_size)\n",
    "\n",
    "seq_len_tensor = torch.full((batch_size,), seq_len, dtype=torch.long)  # or float\n",
    "train_size_tensor = torch.full((batch_size,), train_size, dtype=torch.long)\n",
    "\n",
    "batch = X_train_tensor, y_train_tensor, d, seq_len_tensor, train_size_tensor\n",
    "\n",
    "config = {\n",
    "    \"max_classes\": 10,\n",
    "    \"embed_dim\": 128,\n",
    "    \"col_num_blocks\": 3,\n",
    "    \"col_nhead\": 4,\n",
    "    \"col_num_inds\": 128,\n",
    "    \"row_num_blocks\": 3,\n",
    "    \"row_nhead\": 8,\n",
    "    \"row_num_cls\": 4,\n",
    "    \"row_rope_base\": 100000,\n",
    "    \"icl_num_blocks\": 12,\n",
    "    \"icl_nhead\": 4,\n",
    "    \"ff_factor\": 2,\n",
    "    \"dropout\": 0.0,\n",
    "    \"activation\": \"gelu\",\n",
    "    \"norm_first\": True,\n",
    "}\n",
    "# results = trainer.run_batch(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2367a054",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model,\n",
    "    micro_X,\n",
    "    y,\n",
    "    train_len,\n",
    "    micro_d,\n",
    "    device,\n",
    "    config,\n",
    "    learning_rate=5e-5,\n",
    "    epochs=40,\n",
    "    save_path=\"checkpoints\",\n",
    "):\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    model.to(device)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = ReduceLROnPlateau(\n",
    "        optimizer, mode=\"min\", factor=0.5, patience=5, verbose=True\n",
    "    )\n",
    "\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    best_loss = float(\"inf\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_train = y[:, :train_len].to(device)\n",
    "        y_test = y[:, train_len:].to(device)\n",
    "        micro_X = micro_X.to(device)\n",
    "        micro_d = micro_d.to(device)\n",
    "\n",
    "        pred = model(micro_X, y_train, micro_d)  # (B, test_size, max_classes)\n",
    "        pred = pred.flatten(end_dim=-2)\n",
    "        true = y_test.long().flatten()\n",
    "\n",
    "        loss = criterion(pred, true)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step(loss)\n",
    "\n",
    "        _, predicted = pred.max(1)\n",
    "        accuracy = (predicted == true).sum().item() / true.size(0)\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "        train_accuracies.append(accuracy)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}/{epochs}, Loss: {loss.item():.4f}, \"\n",
    "            f\"lr: {optimizer.param_groups[0]['lr']:.6f}, Accuracy: {accuracy:.6f}\"\n",
    "        )\n",
    "\n",
    "        # ---- Save best checkpoint ----\n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            checkpoint = {\"config\": config, \"state_dict\": model.state_dict()}\n",
    "            torch.save(checkpoint, os.path.join(save_path, \"model_best.pth\"))\n",
    "            print(f\"Best model saved (loss={best_loss:.4f})\")\n",
    "\n",
    "    # ---- Save final model ----\n",
    "    final_ckpt = {\"config\": config, \"state_dict\": model.state_dict()}\n",
    "    torch.save(final_ckpt, os.path.join(save_path, \"model_final.pth\"))\n",
    "    print(\"Final model saved.\")\n",
    "\n",
    "    return train_losses, train_accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0b2a9060",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/D32485/fmproject/.venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train !\n",
      "Out shape :  torch.Size([1, 100, 10])\n",
      "Epoch 1/30, Loss: 0.6672, lr: 0.000050, Accuracy: 0.600000\n",
      "Best model saved (loss=0.6672)\n",
      "Train !\n",
      "Out shape :  torch.Size([1, 100, 10])\n",
      "Epoch 2/30, Loss: 0.6355, lr: 0.000050, Accuracy: 0.620000\n",
      "Best model saved (loss=0.6355)\n",
      "Train !\n",
      "Out shape :  torch.Size([1, 100, 10])\n",
      "Epoch 3/30, Loss: 0.7548, lr: 0.000050, Accuracy: 0.680000\n",
      "Train !\n",
      "Out shape :  torch.Size([1, 100, 10])\n",
      "Epoch 4/30, Loss: 0.5786, lr: 0.000050, Accuracy: 0.700000\n",
      "Best model saved (loss=0.5786)\n",
      "Train !\n",
      "Out shape :  torch.Size([1, 100, 10])\n",
      "Epoch 5/30, Loss: 0.5339, lr: 0.000050, Accuracy: 0.730000\n",
      "Best model saved (loss=0.5339)\n",
      "Train !\n",
      "Out shape :  torch.Size([1, 100, 10])\n",
      "Epoch 6/30, Loss: 0.4680, lr: 0.000050, Accuracy: 0.790000\n",
      "Best model saved (loss=0.4680)\n",
      "Train !\n",
      "Out shape :  torch.Size([1, 100, 10])\n",
      "Epoch 7/30, Loss: 0.4068, lr: 0.000050, Accuracy: 0.810000\n",
      "Best model saved (loss=0.4068)\n",
      "Train !\n",
      "Out shape :  torch.Size([1, 100, 10])\n",
      "Epoch 8/30, Loss: 0.3372, lr: 0.000050, Accuracy: 0.830000\n",
      "Best model saved (loss=0.3372)\n",
      "Train !\n",
      "Out shape :  torch.Size([1, 100, 10])\n",
      "Epoch 9/30, Loss: 0.2413, lr: 0.000050, Accuracy: 0.900000\n",
      "Best model saved (loss=0.2413)\n",
      "Train !\n",
      "Out shape :  torch.Size([1, 100, 10])\n",
      "Epoch 10/30, Loss: 0.1343, lr: 0.000050, Accuracy: 0.950000\n",
      "Best model saved (loss=0.1343)\n",
      "Train !\n",
      "Out shape :  torch.Size([1, 100, 10])\n",
      "Epoch 11/30, Loss: 0.1625, lr: 0.000050, Accuracy: 0.940000\n",
      "Train !\n",
      "Out shape :  torch.Size([1, 100, 10])\n",
      "Epoch 12/30, Loss: 0.3546, lr: 0.000050, Accuracy: 0.880000\n",
      "Train !\n",
      "Out shape :  torch.Size([1, 100, 10])\n",
      "Epoch 13/30, Loss: 0.0880, lr: 0.000050, Accuracy: 0.950000\n",
      "Best model saved (loss=0.0880)\n",
      "Train !\n",
      "Out shape :  torch.Size([1, 100, 10])\n",
      "Epoch 14/30, Loss: 0.0686, lr: 0.000050, Accuracy: 0.960000\n",
      "Best model saved (loss=0.0686)\n",
      "Train !\n",
      "Out shape :  torch.Size([1, 100, 10])\n",
      "Epoch 15/30, Loss: 0.0340, lr: 0.000050, Accuracy: 0.990000\n",
      "Best model saved (loss=0.0340)\n",
      "Train !\n",
      "Out shape :  torch.Size([1, 100, 10])\n",
      "Epoch 16/30, Loss: 0.0335, lr: 0.000050, Accuracy: 1.000000\n",
      "Best model saved (loss=0.0335)\n",
      "Train !\n",
      "Out shape :  torch.Size([1, 100, 10])\n",
      "Epoch 17/30, Loss: 0.0204, lr: 0.000050, Accuracy: 1.000000\n",
      "Best model saved (loss=0.0204)\n",
      "Train !\n",
      "Out shape :  torch.Size([1, 100, 10])\n",
      "Epoch 18/30, Loss: 0.0159, lr: 0.000050, Accuracy: 1.000000\n",
      "Best model saved (loss=0.0159)\n",
      "Train !\n",
      "Out shape :  torch.Size([1, 100, 10])\n",
      "Epoch 19/30, Loss: 0.0067, lr: 0.000050, Accuracy: 1.000000\n",
      "Best model saved (loss=0.0067)\n",
      "Train !\n",
      "Out shape :  torch.Size([1, 100, 10])\n",
      "Epoch 20/30, Loss: 0.0041, lr: 0.000050, Accuracy: 1.000000\n",
      "Best model saved (loss=0.0041)\n",
      "Train !\n",
      "Out shape :  torch.Size([1, 100, 10])\n",
      "Epoch 21/30, Loss: 0.0034, lr: 0.000050, Accuracy: 1.000000\n",
      "Best model saved (loss=0.0034)\n",
      "Train !\n",
      "Out shape :  torch.Size([1, 100, 10])\n",
      "Epoch 22/30, Loss: 0.0024, lr: 0.000050, Accuracy: 1.000000\n",
      "Best model saved (loss=0.0024)\n",
      "Train !\n",
      "Out shape :  torch.Size([1, 100, 10])\n",
      "Epoch 23/30, Loss: 0.0015, lr: 0.000050, Accuracy: 1.000000\n",
      "Best model saved (loss=0.0015)\n",
      "Train !\n",
      "Out shape :  torch.Size([1, 100, 10])\n",
      "Epoch 24/30, Loss: 0.0010, lr: 0.000050, Accuracy: 1.000000\n",
      "Best model saved (loss=0.0010)\n",
      "Train !\n",
      "Out shape :  torch.Size([1, 100, 10])\n",
      "Epoch 25/30, Loss: 0.0009, lr: 0.000050, Accuracy: 1.000000\n",
      "Best model saved (loss=0.0009)\n",
      "Train !\n",
      "Out shape :  torch.Size([1, 100, 10])\n",
      "Epoch 26/30, Loss: 0.0009, lr: 0.000050, Accuracy: 1.000000\n",
      "Train !\n",
      "Out shape :  torch.Size([1, 100, 10])\n",
      "Epoch 27/30, Loss: 0.0008, lr: 0.000050, Accuracy: 1.000000\n",
      "Best model saved (loss=0.0008)\n",
      "Train !\n",
      "Out shape :  torch.Size([1, 100, 10])\n",
      "Epoch 28/30, Loss: 0.0005, lr: 0.000050, Accuracy: 1.000000\n",
      "Best model saved (loss=0.0005)\n",
      "Train !\n",
      "Out shape :  torch.Size([1, 100, 10])\n",
      "Epoch 29/30, Loss: 0.0003, lr: 0.000050, Accuracy: 1.000000\n",
      "Best model saved (loss=0.0003)\n",
      "Train !\n",
      "Out shape :  torch.Size([1, 100, 10])\n",
      "Epoch 30/30, Loss: 0.0002, lr: 0.000050, Accuracy: 1.000000\n",
      "Best model saved (loss=0.0002)\n",
      "Final model saved.\n"
     ]
    }
   ],
   "source": [
    "train_losses, train_accuracies = train(\n",
    "    model=model,\n",
    "    micro_X=X_train_tensor,\n",
    "    y=y_train_tensor,\n",
    "    train_len=300,\n",
    "    micro_d=d,\n",
    "    device=device,\n",
    "    config=config,\n",
    "    learning_rate=5e-5,\n",
    "    epochs=30,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9ce965",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d27f746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load checkpoints custom !\n",
      "Original checkpoint !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/D32485/exercice/tabicl/src/tabicl/sklearn/classifier.py:252: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference !\n",
      "Inference !\n",
      "Inference !\n",
      "Inference !\n",
      "Baseline Accuracy: 0.5400\n"
     ]
    }
   ],
   "source": [
    "clf_original = TabICLClassifier(checkpoint_version=\"original\")\n",
    "clf_original.fit(X_val, y_val)  # this is cheap\n",
    "y_pred_original = clf_original.predict(X_test)  # in-context learning happens here\n",
    "# Compute accuracy\n",
    "acc = accuracy_score(y_test, y_pred_original)\n",
    "print(f\"Baseline Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39abe271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load checkpoints custom !\n",
      "Original checkpoint !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/D32485/exercice/tabicl/src/tabicl/sklearn/classifier.py:252: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference !\n",
      "Inference !\n",
      "Inference !\n",
      "Inference !\n",
      "Baseline Accuracy: 0.6800\n"
     ]
    }
   ],
   "source": [
    "clf_original = TabICLClassifier(checkpoint_version=\"original\")\n",
    "clf_original.fit(X_train, y_train)  # this is cheap\n",
    "y_pred_original = clf_original.predict(X_test)  # in-context learning happens here\n",
    "# Compute accuracy\n",
    "acc = accuracy_score(y_test, y_pred_original)\n",
    "print(f\"Baseline Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1043fc8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load checkpoints custom !\n",
      "Fine-tuned checkpoint !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/D32485/exercice/tabicl/src/tabicl/sklearn/classifier.py:257: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference !\n",
      "Inference !\n",
      "Inference !\n",
      "Inference !\n",
      "Finetuned Accuracy: 0.6200\n"
     ]
    }
   ],
   "source": [
    "clf_finetuned = TabICLClassifier()\n",
    "clf_finetuned.fit(X_val, y_val)  # this is cheap\n",
    "y_pred_finetuned = clf_finetuned.predict(X_test)  # in-context learning happens here\n",
    "# Compute accuracy\n",
    "acc = accuracy_score(y_test, y_pred_finetuned)\n",
    "print(f\"Finetuned Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a42a61f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load checkpoints custom !\n",
      "Fine-tuned checkpoint !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/D32485/exercice/tabicl/src/tabicl/sklearn/classifier.py:257: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference !\n",
      "Inference !\n",
      "Inference !\n",
      "Inference !\n",
      "Finetuned Accuracy: 0.6000\n"
     ]
    }
   ],
   "source": [
    "clf_finetuned = TabICLClassifier()\n",
    "clf_finetuned.fit(X_train, y_train)  # this is cheap\n",
    "y_pred_finetuned = clf_finetuned.predict(X_test)  # in-context learning happens here\n",
    "# Compute accuracy\n",
    "acc = accuracy_score(y_test, y_pred_finetuned)\n",
    "print(f\"Finetuned Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab214bf",
   "metadata": {},
   "source": [
    "### LoRA finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a70b8cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/D32485/exercice/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71332e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80749daf",
   "metadata": {},
   "source": [
    "Apply LoRA to linear layers inside of the attention mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f73f648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn1.linear1\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn1.linear1.base_layer\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn1.linear1.lora_dropout\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn1.linear1.lora_dropout.default\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn1.linear1.lora_A\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn1.linear1.lora_A.default\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn1.linear1.lora_B\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn1.linear1.lora_B.default\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn1.linear1.lora_embedding_A\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn1.linear1.lora_embedding_B\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn1.linear1.lora_magnitude_vector\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn1.linear2\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn1.linear2.base_layer\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn1.linear2.lora_dropout\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn1.linear2.lora_dropout.default\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn1.linear2.lora_A\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn1.linear2.lora_A.default\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn1.linear2.lora_B\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn1.linear2.lora_B.default\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn1.linear2.lora_embedding_A\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn1.linear2.lora_embedding_B\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn1.linear2.lora_magnitude_vector\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn1.attn.out_proj\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn1.attn.out_proj.base_layer\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn1.attn.out_proj.lora_dropout\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn1.attn.out_proj.lora_dropout.default\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn1.attn.out_proj.lora_A\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn1.attn.out_proj.lora_A.default\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn1.attn.out_proj.lora_B\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn1.attn.out_proj.lora_B.default\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn1.attn.out_proj.lora_embedding_A\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn1.attn.out_proj.lora_embedding_B\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn1.attn.out_proj.lora_magnitude_vector\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn2.linear1\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn2.linear1.base_layer\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn2.linear1.lora_dropout\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn2.linear1.lora_dropout.default\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn2.linear1.lora_A\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn2.linear1.lora_A.default\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn2.linear1.lora_B\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn2.linear1.lora_B.default\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn2.linear1.lora_embedding_A\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn2.linear1.lora_embedding_B\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn2.linear1.lora_magnitude_vector\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn2.linear2\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn2.linear2.base_layer\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn2.linear2.lora_dropout\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn2.linear2.lora_dropout.default\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn2.linear2.lora_A\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn2.linear2.lora_A.default\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn2.linear2.lora_B\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn2.linear2.lora_B.default\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn2.linear2.lora_embedding_A\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn2.linear2.lora_embedding_B\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn2.linear2.lora_magnitude_vector\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn2.attn.out_proj\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn2.attn.out_proj.base_layer\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn2.attn.out_proj.lora_dropout\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn2.attn.out_proj.lora_dropout.default\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn2.attn.out_proj.lora_A\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn2.attn.out_proj.lora_A.default\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn2.attn.out_proj.lora_B\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn2.attn.out_proj.lora_B.default\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn2.attn.out_proj.lora_embedding_A\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn2.attn.out_proj.lora_embedding_B\n",
      "LoRA will be applied to: tf_col.blocks.0.multihead_attn2.attn.out_proj.lora_magnitude_vector\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn1.linear1\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn1.linear1.base_layer\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn1.linear1.lora_dropout\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn1.linear1.lora_dropout.default\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn1.linear1.lora_A\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn1.linear1.lora_A.default\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn1.linear1.lora_B\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn1.linear1.lora_B.default\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn1.linear1.lora_embedding_A\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn1.linear1.lora_embedding_B\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn1.linear1.lora_magnitude_vector\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn1.linear2\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn1.linear2.base_layer\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn1.linear2.lora_dropout\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn1.linear2.lora_dropout.default\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn1.linear2.lora_A\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn1.linear2.lora_A.default\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn1.linear2.lora_B\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn1.linear2.lora_B.default\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn1.linear2.lora_embedding_A\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn1.linear2.lora_embedding_B\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn1.linear2.lora_magnitude_vector\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn1.attn.out_proj\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn1.attn.out_proj.base_layer\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn1.attn.out_proj.lora_dropout\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn1.attn.out_proj.lora_dropout.default\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn1.attn.out_proj.lora_A\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn1.attn.out_proj.lora_A.default\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn1.attn.out_proj.lora_B\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn1.attn.out_proj.lora_B.default\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn1.attn.out_proj.lora_embedding_A\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn1.attn.out_proj.lora_embedding_B\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn1.attn.out_proj.lora_magnitude_vector\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn2.linear1\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn2.linear1.base_layer\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn2.linear1.lora_dropout\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn2.linear1.lora_dropout.default\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn2.linear1.lora_A\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn2.linear1.lora_A.default\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn2.linear1.lora_B\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn2.linear1.lora_B.default\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn2.linear1.lora_embedding_A\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn2.linear1.lora_embedding_B\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn2.linear1.lora_magnitude_vector\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn2.linear2\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn2.linear2.base_layer\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn2.linear2.lora_dropout\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn2.linear2.lora_dropout.default\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn2.linear2.lora_A\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn2.linear2.lora_A.default\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn2.linear2.lora_B\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn2.linear2.lora_B.default\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn2.linear2.lora_embedding_A\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn2.linear2.lora_embedding_B\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn2.linear2.lora_magnitude_vector\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn2.attn.out_proj\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn2.attn.out_proj.base_layer\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn2.attn.out_proj.lora_dropout\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn2.attn.out_proj.lora_dropout.default\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn2.attn.out_proj.lora_A\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn2.attn.out_proj.lora_A.default\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn2.attn.out_proj.lora_B\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn2.attn.out_proj.lora_B.default\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn2.attn.out_proj.lora_embedding_A\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn2.attn.out_proj.lora_embedding_B\n",
      "LoRA will be applied to: tf_col.blocks.1.multihead_attn2.attn.out_proj.lora_magnitude_vector\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn1.linear1\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn1.linear1.base_layer\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn1.linear1.lora_dropout\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn1.linear1.lora_dropout.default\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn1.linear1.lora_A\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn1.linear1.lora_A.default\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn1.linear1.lora_B\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn1.linear1.lora_B.default\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn1.linear1.lora_embedding_A\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn1.linear1.lora_embedding_B\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn1.linear1.lora_magnitude_vector\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn1.linear2\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn1.linear2.base_layer\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn1.linear2.lora_dropout\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn1.linear2.lora_dropout.default\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn1.linear2.lora_A\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn1.linear2.lora_A.default\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn1.linear2.lora_B\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn1.linear2.lora_B.default\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn1.linear2.lora_embedding_A\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn1.linear2.lora_embedding_B\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn1.linear2.lora_magnitude_vector\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn1.attn.out_proj\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn1.attn.out_proj.base_layer\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn1.attn.out_proj.lora_dropout\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn1.attn.out_proj.lora_dropout.default\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn1.attn.out_proj.lora_A\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn1.attn.out_proj.lora_A.default\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn1.attn.out_proj.lora_B\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn1.attn.out_proj.lora_B.default\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn1.attn.out_proj.lora_embedding_A\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn1.attn.out_proj.lora_embedding_B\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn1.attn.out_proj.lora_magnitude_vector\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn2.linear1\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn2.linear1.base_layer\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn2.linear1.lora_dropout\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn2.linear1.lora_dropout.default\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn2.linear1.lora_A\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn2.linear1.lora_A.default\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn2.linear1.lora_B\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn2.linear1.lora_B.default\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn2.linear1.lora_embedding_A\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn2.linear1.lora_embedding_B\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn2.linear1.lora_magnitude_vector\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn2.linear2\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn2.linear2.base_layer\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn2.linear2.lora_dropout\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn2.linear2.lora_dropout.default\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn2.linear2.lora_A\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn2.linear2.lora_A.default\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn2.linear2.lora_B\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn2.linear2.lora_B.default\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn2.linear2.lora_embedding_A\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn2.linear2.lora_embedding_B\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn2.linear2.lora_magnitude_vector\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn2.attn.out_proj\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn2.attn.out_proj.base_layer\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn2.attn.out_proj.lora_dropout\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn2.attn.out_proj.lora_dropout.default\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn2.attn.out_proj.lora_A\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn2.attn.out_proj.lora_A.default\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn2.attn.out_proj.lora_B\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn2.attn.out_proj.lora_B.default\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn2.attn.out_proj.lora_embedding_A\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn2.attn.out_proj.lora_embedding_B\n",
      "LoRA will be applied to: tf_col.blocks.2.multihead_attn2.attn.out_proj.lora_magnitude_vector\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/D32485/exercice/.venv/lib/python3.12/site-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/home/D32485/exercice/.venv/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"linear1\", \"linear2\", \"attn.out_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\",\n",
    ")\n",
    "\n",
    "model.col_embedder = get_peft_model(model.col_embedder, lora_config)\n",
    "\n",
    "for name, module in model.col_embedder.named_modules():\n",
    "    if any(target in name for target in lora_config.target_modules):\n",
    "        print(\"LoRA will be applied to in column embedder:\", name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4958b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
